{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus:  the sonnets <NEXT> by william shakespeare <NEXT>  <NEXT>                       <NEXT>   from fairest creatures we desire increase ,  <NEXT>   that thereby beautys rose might never die ,  <NEXT>   but as the riper should by time decease ,  <NEXT>   his tender heir might bear his memory <NEXT>   but thou contracted to thine own bright eyes ,  <NEXT>   feedst thy lights flame with self substantial fu\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join('data','shakespeare.txt'),'r') as f:\n",
    "    corpus=f.read().lower()\n",
    "\n",
    "import re,string\n",
    "\n",
    "corpus=re.sub('-',' ',corpus)\n",
    "corpus=re.sub(f'[{re.escape(string.digits)}]','',corpus)\n",
    "corpus=re.sub('[:\\';\\\"?<>/&*^!`|\\(\\)\\[\\]]','',corpus)\n",
    "corpus=re.sub('\\n',' <NEXT> ',corpus)\n",
    "corpus=re.sub(f',',' , ',corpus)\n",
    "corpus=re.sub(f'\\.',' . ',corpus)\n",
    "print('Corpus: ',corpus[:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus length: 6429217\n",
      "Vocab size: 25372\n",
      "Vocabulary: ['abel', 'abbreviated', 'abbots', 'abbominable', 'abbeys', 'abates', 'abatements', 'abashd', 'abaissiez', 'aarons']\n"
     ]
    }
   ],
   "source": [
    "print(f'Corpus length: {len(corpus)}')\n",
    "import json\n",
    "\n",
    "with open('hyperparameters.json','r') as f:\n",
    "    params=json.load(f)\n",
    "\n",
    "max_tokens=params['max_tokens']\n",
    "max_sequence_length=params['max_sequence_length']\n",
    "step_size=params['step_size']\n",
    "batch_size=params['batch_size']\n",
    "lstm_units=params['lstm_units']\n",
    "learning_rate=params['learning_rate']\n",
    "embedding_dim=params['embedding_dim']\n",
    "\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "vectorize_layer=TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode='int',\n",
    "    standardize=None,\n",
    "    pad_to_max_tokens=True\n",
    ")\n",
    "vectorize_layer.adapt([corpus],batch_size=256)\n",
    "\n",
    "print(f'Vocab size: {len(vectorize_layer.get_vocabulary())}')\n",
    "print(f'Vocabulary: {vectorize_layer.get_vocabulary()[-10:]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: those hours that with gentle work\n",
      "Sequence->id: [201 681  14  17 280 543]\n",
      "Id->sequence: wall pol my is bid officer\n"
     ]
    }
   ],
   "source": [
    "def sequences2ids(sequence):\n",
    "    return vectorize_layer(sequence)\n",
    "\n",
    "def ids2sequences(ids):\n",
    "    decode=[]\n",
    "    if type(ids)==int:\n",
    "        ids=[ids]\n",
    "    for id in ids:\n",
    "        decode.append(vectorize_layer.get_vocabulary()[id])\n",
    "    decode=' '.join(decode)\n",
    "    decode=re.sub(' <NEXT> ',' \\n ',decode)\n",
    "    decode=re.sub(' , ',', ',decode)\n",
    "    decode=re.sub(' . ','. ',decode)\n",
    "    return decode\n",
    "\n",
    "print('Input: those hours that with gentle work')\n",
    "print(f'Sequence->id: {sequences2ids(\"those hours that with gentle work\").numpy()}')\n",
    "print(f'Id->sequence: {ids2sequences([1253, 1123,   12,   15,  308,  951])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus shape: (1167869,)\n"
     ]
    }
   ],
   "source": [
    "corpus=corpus.split()\n",
    "corpus=sequences2ids(corpus)\n",
    "corpus=corpus.numpy().reshape(-1)\n",
    "print(f'Corpus shape: {corpus.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded Input: [   5 8112    2   38  818 2195    2    2]...\n",
      "Decoded Input: the sonnets \n",
      " by william shakespeare \n",
      " <NEXT>...\n",
      "Encoded Target: 2\n",
      "Decoded Target: <NEXT>\n",
      "\n",
      "Dataset size: 389287\n",
      "Input Sequences shape: (389287, 8)\n",
      "Target words shape: (389287,)\n"
     ]
    }
   ],
   "source": [
    "input_sequences=[]\n",
    "target=[]\n",
    "for i in range(0,len(corpus)-max_sequence_length,step_size):\n",
    "    input_sequences.append(corpus[i:i+max_sequence_length])\n",
    "    target.append(corpus[i+max_sequence_length])\n",
    "\n",
    "input_sequences=np.array(input_sequences)\n",
    "target=np.array(target)\n",
    "\n",
    "print(f'Encoded Input: {input_sequences[0][:10]}...')\n",
    "print(f'Decoded Input: {ids2sequences(input_sequences[0][:10])}...')\n",
    "print(f'Encoded Target: {target[0]}')\n",
    "print(f'Decoded Target: {ids2sequences(target[:1])}\\n')\n",
    "\n",
    "print(f'Dataset size: {len(input_sequences)}')\n",
    "print(f'Input Sequences shape: {input_sequences.shape}')\n",
    "print(f'Target words shape: {target.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset size: 761\n",
      "Input shape (with batches): (512, 8)\n",
      "Output shape (with batches): (512,)\n"
     ]
    }
   ],
   "source": [
    "data=tf.data.Dataset.from_tensor_slices((input_sequences,target))\n",
    "data=data.cache()\n",
    "data=data.shuffle(1000)\n",
    "data=data.batch(batch_size)\n",
    "data=data.prefetch(tf.data.AUTOTUNE)\n",
    "data_iterator=data.as_numpy_iterator()\n",
    "\n",
    "print(f'Dataset size: {len(data)}')\n",
    "_=data_iterator.next()\n",
    "print(f'Input shape (with batches): {_[0].shape}')\n",
    "print(f'Output shape (with batches): {_[1].shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model,Input\n",
    "from tensorflow.keras.layers import LSTM,Bidirectional,Dropout,Embedding,Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    inputs=Input(shape=(max_sequence_length,))\n",
    "    x=Embedding(max_tokens,embedding_dim)(inputs)\n",
    "    x=Bidirectional(\n",
    "        LSTM(lstm_units,return_sequences=True,dropout=.2)\n",
    "    )(x)\n",
    "    x=Bidirectional(\n",
    "        LSTM(lstm_units,dropout=.2,return_sequences=True)\n",
    "    )(x)\n",
    "    x=LSTM(lstm_units//2)(x)\n",
    "    x=Dense(max_tokens,activation='softmax')(x)\n",
    "\n",
    "    model=Model(inputs=inputs,outputs=x,name='sonnet_model')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sonnet_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 8)]               0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, 8, 32)             811904    \n",
      "                                                                 \n",
      " lstm_4 (LSTM)               (None, 8, 32)             8320      \n",
      "                                                                 \n",
      " lstm_5 (LSTM)               (None, 32)                8320      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 25372)             837276    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,665,820\n",
      "Trainable params: 1,665,820\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "sonnet_model=build_model()\n",
    "sonnet_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 25372), dtype=float32, numpy=\n",
       "array([[3.9413542e-05, 3.9406412e-05, 3.9420571e-05, ..., 3.9412676e-05,\n",
       "        3.9414084e-05, 3.9415605e-05]], dtype=float32)>"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sonnet_model(data_iterator.next()[0][:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "913/913 [==============================] - 151s 165ms/step - loss: 5.9681 - accuracy: 0.1038\n"
     ]
    }
   ],
   "source": [
    "history=sonnet_model.fit(data,epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sonnet_model=tf.keras.models.load_model('models/sonnet_model.h5',compile=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def check_prediction(nums):\n",
    "    correct=0\n",
    "    print(f'Input\\t------->\\tPrediction : Actual')\n",
    "    for i in range(nums):\n",
    "        idx=random.randint(0,len(input_sequences)-1)\n",
    "        y_pred=sonnet_model(input_sequences[idx:idx+1],training=False)\n",
    "        y_pred=tf.argmax(y_pred,axis=-1).numpy().item()\n",
    "        print(f'{ids2sequences(input_sequences[idx])} --> {ids2sequences(y_pred)} : {ids2sequences(target[idx:idx+1])}')\n",
    "        if y_pred==target[idx]:correct+=1\n",
    "    print(f'Accuracy: {(correct/nums)*100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input\t------->\tPrediction : Actual\n",
      "know you do, and have found it --> <NEXT> : .\n",
      ". hell beat aufidius head below his knee --> <NEXT> : <NEXT>\n",
      "in solemn talk. \n",
      " corin. that --> is : is\n",
      ". your cares set up do not pluck --> him : my\n",
      "monstrous arrogance thou liest, thou thread , --> my : thou\n",
      "to his goodness \n",
      " the model of our --> great : chaste\n",
      "a word. \n",
      " volumnius. what says --> thou : my\n",
      "all filld up with guts and midriff . --> <NEXT> : <NEXT>\n",
      "affectiond ass that cons state without book and --> <NEXT> : <NEXT>\n",
      "ireland, who removd, \n",
      " earl surrey --> hath : was\n",
      "Accuracy: 40.00%\n"
     ]
    }
   ],
   "source": [
    "check_prediction(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z)/sum(np.exp(z))\n",
    "\n",
    "def sample(conditional_probability,temperature=1.0):\n",
    "    conditional_probability = np.asarray(conditional_probability).astype(\"float64\")\n",
    "    conditional_probability = np.log(conditional_probability) / temperature\n",
    "    reweighted_conditional_probability = softmax(conditional_probability)\n",
    "    probas = np.random.multinomial(1, reweighted_conditional_probability, 1)\n",
    "    return np.argmax(probas)\n",
    "\n",
    "def generate_sequence(initial_seed,steps):\n",
    "    gen=0\n",
    "    encoded_seq=None\n",
    "    if initial_seed=='':\n",
    "        idx=random.randint(0,max_tokens-1)\n",
    "        initial_seed=ids2sequences(idx)\n",
    "    encoded_seq=list(sequences2ids(initial_seed).numpy())\n",
    "    while gen!=steps:\n",
    "        gen+=1\n",
    "        input_seq=np.zeros((1,max_sequence_length))\n",
    "        last_sequence=encoded_seq[len(encoded_seq)-max_sequence_length:]\n",
    "        for idx,enc in enumerate(last_sequence):\n",
    "            input_seq[:,idx]=enc\n",
    "        y_pred=sonnet_model(input_seq,training=False).numpy().flatten()\n",
    "        y_pred=sample(y_pred)\n",
    "        encoded_seq.append(y_pred)\n",
    "    return ids2sequences(encoded_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "as thou you shall ., thou, only youth, \n",
      " and then steal up from expectation most touches, \n",
      " for her spur or even occasion sick as our good eyes \n",
      " with sorrow of love what was you so. \n",
      " welcome, camillo. must hunt you when nature would, you see \n",
      " that hit my censure let me utter and thyself \n",
      " not in me and leave me so. \n",
      " camillo. he may not use thee that. did stand for me \n",
      " shamd oaks and, fool, folly, or, glazed, <NEXT>\n"
     ]
    }
   ],
   "source": [
    "print(generate_sequence('as thou you shall',100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2word={}\n",
    "word2id={}\n",
    "for idx,word in enumerate(vectorize_layer.get_vocabulary()):\n",
    "    id2word[idx]=word\n",
    "    word2id[word]=idx\n",
    "\n",
    "import pickle\n",
    "\n",
    "with open('models/vocabulary.h5','wb') as f:\n",
    "    pickle.dump(id2word,f)\n",
    "    pickle.dump(word2id,f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b97ecd702dc84eaaa004cc29d5d591a9e7c0154bc6bf90140e730ee222020015"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
